import os
import uuid
from typing import TypedDict

from getpass import getpass
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchTool
from langchain.agents import create_agent
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, interrupt

# --- (Optional) Set API Keys ---
# os.environ["OPENAI_API_KEY"] = getpass("Enter your OpenAI API key: ")
# os.environ["TAVILY_API_KEY"] = getpass("Enter your Tavily API key: ")


# --- 1. Create the Stateful Agent (with its own memory) ---

# Define tools for the agent
tools = [TavilySearchTool(max_results=2)]

# Set up the prompt for the agent
SYSTEM_PROMPT = """You are a helpful assistant. You will be given a user query and must generate a response.
You have access to tools. You must use them if the query requires outside information.

You will also be given feedback on your previous generations. You must use this feedback
and your memory of the conversation to generate a new, improved response that
incorporates the user's modifications."""

# Create the LLM
model = ChatOpenAI(model="gpt-4o", temperature=0)

# Create the agent's internal memory
# We will use this *same* checkpointer for the graph
checkpointer = MemorySaver()

# Create the agent_runnable using the latest `create_agent` function
# This agent is now stateful and manages its own conversation history
agent_runnable = create_agent(
    model=model,
    system_prompt=SYSTEM_PROMPT,
    tools=tools,
    checkpointer=checkpointer,  # This is the agent's memory
)


# --- 2. Define the Graph State ---

class AgentState(TypedDict):
    """
    The state for our graph.

    Attributes:
        agent_output: The last text generated by the agent.
        user_feedback: The last input from the user (can be the initial query or modifications).
    """
    agent_output: str
    user_feedback: str


# --- 3. Define Graph Nodes ---

def agent_node(state: AgentState):
    """
    This node runs the stateful agent.

    The agent's internal checkpointer will automatically load the full
    conversation history. We just need to pass the *latest* user message.
    """
    print("--- ü§ñ AGENT NODE RUNNING ---")
    
    # Get the user's latest input
    user_input = state["user_feedback"]
    
    # Invoke the agent. The config is passed by the graph.
    # The agent_runnable will automatically load its memory using the thread_id
    response = agent_runnable.invoke(
        {"messages": [HumanMessage(content=user_input)]}
    )
    
    # Get the agent's new response (the last message)
    new_output = response['messages'][-1].content
    
    print(f"--- Agent Generated: '{new_output}' ---")
    
    # Update the state with the agent's new output
    return {"agent_output": new_output}


def user_review_node(state: AgentState):
    """
    This node *pauses* the graph to ask for human review.
    It uses the `interrupt()` function as shown in your reference.
    """
    print("--- ‚è∏Ô∏è PAUSING FOR HUMAN REVIEW ---")
    
    # Get the agent's output that we want the user to review
    output_to_review = state["agent_output"]
    
    # This is the magic:
    # 1. `interrupt()` pauses the graph.
    # 2. It sends the dictionary back to the main script.
    # 3. The graph *waits* until `graph.invoke(Command(resume=...), ...)` is called.
    # 4. The value passed to `resume` will be returned here and stored in `feedback`.
    feedback = interrupt({
        "instruction": "Please review the agent's output. Type 'approve' to finish, or provide feedback for modifications.",
        "content_to_review": output_to_review
    })
    
    print(f"---  resumed with feedback: '{feedback}' ---")
    
    # Save the user's feedback to the state
    return {"user_feedback": feedback}


# --- 4. Define Conditional Edge (The Router) ---

def decide_to_finish(state: AgentState):
    """
    This function is the "conditional edge". It checks the user_feedback.
    
    If "approve", it routes to END.
    Otherwise, it routes back to the "agent" node to loop.
    """
    print("--- üßê DECISION NODE ---")
    
    if state["user_feedback"].lower() == "approve":
        print("--- Decision: Approved. Ending graph. ---")
        return END
    else:
        print("--- Decision: Modifications received. Looping back to agent. ---")
        return "agent"  # The name of the agent node


# --- 5. Assemble the Graph ---

print("Assembling the graph...")
builder = StateGraph(AgentState)

# Add the nodes
builder.add_node("agent", agent_node)
builder.add_node("review", user_review_node)

# Set the entry point
# The user's *first* invoke will provide the initial "user_feedback" (the query)
builder.set_entry_point("agent")

# Add the edges
# After the agent runs, it *always* goes to the review node
builder.add_edge("agent", "review")

# After the review node, we make a decision
builder.add_conditional_edges(
    "review",  # The node we just came from
    decide_to_finish,  # The function to call to decide where to go
    {
        "agent": "agent",  # If the function returns "agent", go to the "agent" node
        END: END           # If the function returns END, go to END
    }
)

# Compile the graph
# We use the *same checkpointer* so the graph and agent share memory
graph = builder.compile(checkpointer=checkpointer)

print("Graph compiled. Ready to run.")

# --- 6. Run the Graph Loop ---

# Create a unique ID for this conversation
thread_id = str(uuid.uuid4())
config = {"configurable": {"thread_id": thread_id}}

# 1. Get the initial query from the user
initial_query = input("Please enter your initial query: ")
print("\n====================================")

# 2. Start the graph for the first time
# We provide the initial state: the user's first query
initial_state = {"user_feedback": initial_query}
current_state = graph.invoke(initial_state, config=config)

# 3. Start the loop to handle interruptions
while "__interrupt__" in current_state:
    # 3.1. The graph is paused. Get the interruption data
    interrupt_data = current_state["__interrupt__"]
    
    # 3.2. Print the instructions and the content for the user
    print("\n--- ‚ùó HUMAN-IN-THE-LOOP ---")
    print(f"Instruction: {interrupt_data['instruction']}")
    print("---------------------------------")
    print(f"Agent Output:\n{interrupt_data['content_to_review']}")
    print("---------------------------------")
    
    # 3.3. Get the user's feedback
    user_feedback = input("Your feedback ('approve' to end): ")
    print("\n====================================")
    
    # 3.4. Resume the graph with the user's feedback
    # We pass the feedback string in the Command(resume=...)
    current_state = graph.invoke(
        Command(resume=user_feedback),
        config=config,
    )

# 4. If the loop breaks, the graph has finished
print("\n--- ‚úÖ Graph Finished ---")
print("Final approved state:")
print(current_state)
